import Docker from "dockerode";

// 1. Setup Docker ‡πÅ‡∏ö‡∏ö‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö
const isWindows = process.platform === "win32";
const docker = new Docker(isWindows ? { host: "127.0.0.1", port: 2375 } : { socketPath: "/var/run/docker.sock" });

export interface ChatInstanceResult {
  containerId: string;
  port: string;
  model: string;
}

/**
 * 2. Pull Image ‡πÅ‡∏ö‡∏ö‡∏¢‡πà‡∏≠ (‡∏ï‡∏±‡∏î Progress bar ‡∏£‡∏Å‡πÜ ‡∏ó‡∏¥‡πâ‡∏á)
 */
async function ensureOllamaImage(): Promise<void> {
  const imageName = "ollama/ollama";
  try {
    await docker.getImage(imageName).inspect();
  } catch (error) {
    console.log(`üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Image ${imageName}...`);
    // ‡πÉ‡∏ä‡πâ followProgress ‡πÅ‡∏ö‡∏ö‡∏°‡∏¥‡∏ô‡∏¥‡∏°‡∏≠‡∏• ‡πÅ‡∏Ñ‡πà‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏à‡∏ö
    await new Promise<void>((resolve, reject) => {
      docker.pull(imageName, (err: any, stream: any) => {
        if (err) return reject(err);
        docker.modem.followProgress(stream, (err) => err ? reject(err) : resolve());
      });
    });
    console.log(`‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Image ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à`);
  }
}

/**
 * ‡∏™‡∏£‡πâ‡∏≤‡∏á Chat Instance (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ Pull Model ‡πÄ‡∏õ‡πá‡∏ô HTTP API)
 */
export async function createChatInstance(modelName: string = "qwen:0.5b"): Promise<ChatInstanceResult> {
  console.log(`üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏ä‡∏ó: ${modelName}`);

  try {
    await ensureOllamaImage();

    // 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞ Start Container
    const container = await docker.createContainer({
      Image: "ollama/ollama",
      Tty: true,
      HostConfig: {
        PortBindings: { "11434/tcp": [{ HostPort: "" }] }, // ‡∏™‡∏∏‡πà‡∏° Port
        Memory: 1024 * 1024 * 1024,
      },
    });

    await container.start();

    // ‡∏î‡∏∂‡∏á Port
    const data = await container.inspect();
    const hostPort = data.NetworkSettings.Ports["11434/tcp"]?.[0]?.HostPort;
    if (!hostPort) throw new Error("‡∏´‡∏≤ Port ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠");

    console.log(`‚úÖ Container ‡∏£‡∏±‡∏ô‡∏ó‡∏µ‡πà Port: ${hostPort}`);

    // 4. ‡∏£‡∏≠ Service ‡∏û‡∏£‡πâ‡∏≠‡∏°
    await waitForOllama(hostPort);

    // 5. üî• ‡πÑ‡∏Æ‡πÑ‡∏•‡∏ó‡πå: ‡πÉ‡∏ä‡πâ HTTP API Pull ‡πÅ‡∏ó‡∏ô exec (‡∏•‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏™‡∏ï‡∏£‡∏µ‡∏° Docker 101 ‡∏ó‡∏¥‡πâ‡∏á‡πÑ‡∏õ‡πÄ‡∏•‡∏¢)
    console.log(`‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ Container ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• ${modelName}...`);
    
    const response = await fetch(`http://localhost:${hostPort}/api/pull`, {
      method: "POST",
      body: JSON.stringify({ name: modelName }),
    });

    if (!response.body) throw new Error("Failed to pull model");

    // ‡∏≠‡πà‡∏≤‡∏ô Stream ‡∏à‡∏≤‡∏Å HTTP Response (‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤ Docker Stream ‡∏°‡∏≤‡∏Å)
    const reader = response.body.getReader();
    while (true) {
      const { done } = await reader.read();
      if (done) break;
      // ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏î‡∏π log ‡πÉ‡∏´‡πâ parse chunk ‡πÄ‡∏õ‡πá‡∏ô text ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
    }

    console.log(`üéâ ‡πÇ‡∏°‡πÄ‡∏î‡∏• ${modelName} ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!`);

    return { containerId: container.id, port: hostPort, model: modelName };

  } catch (error) {
    console.error("‚ùå Error:", error);
    throw error;
  }
}

// Helper: ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏° (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡πÅ‡∏ï‡πà‡∏ï‡∏±‡∏î comment ‡∏£‡∏Å‡πÜ)
async function waitForOllama(port: string): Promise<void> {
  for (let i = 0; i < 30; i++) {
    try {
      if ((await fetch(`http://localhost:${port}/api/tags`)).ok) return;
    } catch (e) {}
    await new Promise((r) => setTimeout(r, 1000));
  }
  throw new Error("Ollama start timeout");
}