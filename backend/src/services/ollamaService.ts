import Docker from "dockerode";

const OLLAMA_VOLUME = process.env.OLLAMA_VOLUME || "ollama-models";

// 1. Setup Docker ‡πÅ‡∏ö‡∏ö‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö
const isWindows = process.platform === "win32";
const docker = new Docker(isWindows ? { host: "127.0.0.1", port: 2375 } : { socketPath: "/var/run/docker.sock" });

export interface ChatInstanceResult {
  containerId: string;
  port: string;
  model: string;
}


async function ensureOllamaImage(): Promise<void> {
  const imageName = "ollama/ollama";
  try {
    await docker.getImage(imageName).inspect();
  } catch (error) {
    console.log(`üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Image ${imageName}...`);
    // ‡πÉ‡∏ä‡πâ followProgress ‡πÅ‡∏ö‡∏ö‡∏°‡∏¥‡∏ô‡∏¥‡∏°‡∏≠‡∏• ‡πÅ‡∏Ñ‡πà‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏à‡∏ö
    await new Promise<void>((resolve, reject) => {
      docker.pull(imageName, (err: any, stream: any) => {
        if (err) return reject(err);
        docker.modem.followProgress(stream, (err) => (err ? reject(err) : resolve()));
      });
    });
    console.log(`‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Image ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à`);
  }
}


export async function createOllamaInstance(modelName: string = "qwen:0.5b"): Promise<ChatInstanceResult> {
  console.log(`üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏ä‡∏ó: ${modelName}`);

  try {
    await ensureOllamaImage();

    // 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞ Start Container
    const container = await docker.createContainer({
      Image: "ollama/ollama",
      Tty: true,
      HostConfig: {
        PortBindings: { "11434/tcp": [{ HostPort: "" }] }, // ‡∏™‡∏∏‡πà‡∏° Port
        Memory: 1024 * 1024 * 1024,
        Binds: [`${OLLAMA_VOLUME}:/root/.ollama:ro`],
      },
    });

    await container.start();

    // ‡∏î‡∏∂‡∏á Port
    const data = await container.inspect();
    const hostPort = data.NetworkSettings.Ports["11434/tcp"]?.[0]?.HostPort;
    if (!hostPort) throw new Error("‡∏´‡∏≤ Port ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠");

    console.log(`‚úÖ Container ‡∏£‡∏±‡∏ô‡∏ó‡∏µ‡πà Port: ${hostPort}`);

    // 4. ‡∏£‡∏≠ Service ‡∏û‡∏£‡πâ‡∏≠‡∏°
    await waitForOllama(hostPort);

    console.log(`üéâ ‡πÇ‡∏°‡πÄ‡∏î‡∏• ${modelName} ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!`);

    return { containerId: container.id, port: hostPort, model: modelName };
  } catch (error) {
    console.error("‚ùå Error:", error);
    throw error;
  }
}

async function waitForOllama(port: string): Promise<void> {
  for (let i = 0; i < 30; i++) {
    try {
      if ((await fetch(`http://localhost:${port}/api/tags`)).ok) return;
    } catch (e) {}
    await new Promise((r) => setTimeout(r, 1000));
  }
  throw new Error("Ollama start timeout");
}
